{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437385b-ccf4-4e9a-8ac9-e6b6e50e1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ortega\n",
    "from math import radians,cos,sin,asin,sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, MultiPoint\n",
    "from shapely import wkt\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "from pathlib import Path\n",
    "\n",
    "# !pip install --upgrade ortega\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ce985-908e-444f-b609-eeb92b1321ab",
   "metadata": {},
   "source": [
    "# Read in GPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b2f88-5995-4ef0-b180-99f4dc809856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/tiger_leopard_env.csv')\n",
    "df['Time_LMT'] = pd.to_datetime(df['Time_LMT'])\n",
    "df.groupby(['idcollar'])[['slope', 'sb', 'bt']].agg(['min','max','count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07c3c2a-7647-4faf-b26e-8fb6056e6edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Time_LMT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idcollar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31898</th>\n",
       "      <td>2018-09-27 22:15:00</td>\n",
       "      <td>2020-02-18 06:02:00</td>\n",
       "      <td>10717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31899</th>\n",
       "      <td>2019-03-10 21:30:00</td>\n",
       "      <td>2020-02-22 12:01:00</td>\n",
       "      <td>8034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>2019-10-09 10:00:00</td>\n",
       "      <td>2020-03-28 03:00:00</td>\n",
       "      <td>3470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37822</th>\n",
       "      <td>2019-10-16 07:00:00</td>\n",
       "      <td>2020-03-07 09:01:00</td>\n",
       "      <td>2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37823</th>\n",
       "      <td>2019-10-19 23:00:00</td>\n",
       "      <td>2021-01-06 15:42:00</td>\n",
       "      <td>9507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131343</th>\n",
       "      <td>2018-08-30 09:00:00</td>\n",
       "      <td>2020-11-16 07:00:00</td>\n",
       "      <td>18584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229011</th>\n",
       "      <td>2017-12-20 07:00:00</td>\n",
       "      <td>2019-01-30 19:00:00</td>\n",
       "      <td>9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229012</th>\n",
       "      <td>2019-12-21 08:01:00</td>\n",
       "      <td>2020-12-28 06:00:00</td>\n",
       "      <td>7164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229022</th>\n",
       "      <td>2019-11-04 10:00:00</td>\n",
       "      <td>2020-12-09 15:00:00</td>\n",
       "      <td>7786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229032</th>\n",
       "      <td>2019-09-12 22:00:00</td>\n",
       "      <td>2020-10-02 02:00:00</td>\n",
       "      <td>8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229041</th>\n",
       "      <td>2018-08-20 20:00:00</td>\n",
       "      <td>2019-09-22 08:02:00</td>\n",
       "      <td>6151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time_LMT                           \n",
       "                         min                 max  count\n",
       "idcollar                                               \n",
       "31898    2018-09-27 22:15:00 2020-02-18 06:02:00  10717\n",
       "31899    2019-03-10 21:30:00 2020-02-22 12:01:00   8034\n",
       "37821    2019-10-09 10:00:00 2020-03-28 03:00:00   3470\n",
       "37822    2019-10-16 07:00:00 2020-03-07 09:01:00   2257\n",
       "37823    2019-10-19 23:00:00 2021-01-06 15:42:00   9507\n",
       "131343   2018-08-30 09:00:00 2020-11-16 07:00:00  18584\n",
       "229011   2017-12-20 07:00:00 2019-01-30 19:00:00   9504\n",
       "229012   2019-12-21 08:01:00 2020-12-28 06:00:00   7164\n",
       "229022   2019-11-04 10:00:00 2020-12-09 15:00:00   7786\n",
       "229032   2019-09-12 22:00:00 2020-10-02 02:00:00   8625\n",
       "229041   2018-08-20 20:00:00 2019-09-22 08:02:00   6151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['idcollar'])[['Time_LMT']].agg(['min','max','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4424cd59-db29-4077-8d1a-f22c5177b43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'event_id', 'idcollar', 'latitude', 'longitude', 'DSM', 'slope',\n",
       "       'temperature', 'Time_UTC', 'Time_LMT', 'stepLength', 'duration',\n",
       "       'speed', 'year', 'month', 'day', 'hour', 'daynight', 'drywet',\n",
       "       'year/month', 'proj_lon', 'proj_lat', 'stream', 'road_trail', 'trail',\n",
       "       'road', 'village', 'NDVI', 'altitude', 'tg', 'sb', 'gr', 'bt', 'prey',\n",
       "       'Behavior_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23aa94-ba39-4193-b643-647ddb84cdae",
   "metadata": {},
   "source": [
    "# Compute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f95795-331f-455b-814e-f2e74aeaf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_turning_angles(df):\n",
    "    \"\"\"\n",
    "    Computes turning angles for each individual and adds them to the dataframe.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Avoid modifying the original dataframe\n",
    "\n",
    "    # Compute displacement vectors\n",
    "    df['dx'] = df.groupby('idcollar')['proj_lon'].diff()\n",
    "    df['dy'] = df.groupby('idcollar')['proj_lat'].diff()\n",
    "\n",
    "    # Compute movement directions (angles in radians)\n",
    "    df['direction'] = np.arctan2(df['dy'], df['dx'])\n",
    "\n",
    "    # Compute turning angles (change in direction between consecutive steps)\n",
    "    df['turning_angle'] = df.groupby('idcollar')['direction'].diff()\n",
    "\n",
    "    # Normalize turning angles to range [-π, π]\n",
    "    df['turning_angle'] = df['turning_angle'].apply(lambda x: (x + np.pi) % (2 * np.pi) - np.pi)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867125a6-55c4-48db-8355-d4c308cac01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning angles computed and saved to updated CSV file.\n"
     ]
    }
   ],
   "source": [
    "df = compute_turning_angles(df)  # Add turning angle column\n",
    "df.to_csv('../../data/tiger_leopard_env_tg.csv')\n",
    "print(\"Turning angles computed and saved to updated CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b460c56-34f6-46ee-9b14-c7967acf73bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">stepLength</th>\n",
       "      <th colspan=\"5\" halign=\"left\">turning_angle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idcollar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31898</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.225146</td>\n",
       "      <td>10717</td>\n",
       "      <td>0.143853</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141593</td>\n",
       "      <td>10715</td>\n",
       "      <td>-0.020977</td>\n",
       "      <td>-0.033245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31899</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.060886</td>\n",
       "      <td>8034</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141178</td>\n",
       "      <td>8032</td>\n",
       "      <td>0.037870</td>\n",
       "      <td>0.047588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.330141</td>\n",
       "      <td>3470</td>\n",
       "      <td>0.114664</td>\n",
       "      <td>0.018964</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.139853</td>\n",
       "      <td>3468</td>\n",
       "      <td>-0.002083</td>\n",
       "      <td>-0.038864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37822</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.296052</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.238676</td>\n",
       "      <td>0.041692</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.140194</td>\n",
       "      <td>2255</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37823</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.557510</td>\n",
       "      <td>9507</td>\n",
       "      <td>0.308503</td>\n",
       "      <td>0.044157</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.140023</td>\n",
       "      <td>9505</td>\n",
       "      <td>-0.020415</td>\n",
       "      <td>-0.025846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.586030</td>\n",
       "      <td>18584</td>\n",
       "      <td>0.281316</td>\n",
       "      <td>0.038141</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141294</td>\n",
       "      <td>18582</td>\n",
       "      <td>0.035077</td>\n",
       "      <td>0.040854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.692342</td>\n",
       "      <td>9504</td>\n",
       "      <td>0.142698</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141526</td>\n",
       "      <td>9502</td>\n",
       "      <td>-0.015796</td>\n",
       "      <td>-0.021074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.289849</td>\n",
       "      <td>7164</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.021323</td>\n",
       "      <td>-3.141583</td>\n",
       "      <td>3.141530</td>\n",
       "      <td>7162</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.007033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229022</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.916479</td>\n",
       "      <td>7786</td>\n",
       "      <td>0.281002</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141533</td>\n",
       "      <td>7784</td>\n",
       "      <td>0.015167</td>\n",
       "      <td>-0.001848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229032</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.176556</td>\n",
       "      <td>8625</td>\n",
       "      <td>0.213572</td>\n",
       "      <td>0.029240</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141446</td>\n",
       "      <td>8623</td>\n",
       "      <td>-0.021255</td>\n",
       "      <td>-0.036740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.419672</td>\n",
       "      <td>6151</td>\n",
       "      <td>0.391094</td>\n",
       "      <td>0.029653</td>\n",
       "      <td>-3.141593</td>\n",
       "      <td>3.141226</td>\n",
       "      <td>6149</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>0.040875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stepLength                                       turning_angle  \\\n",
       "                min        max  count      mean    median           min   \n",
       "idcollar                                                                  \n",
       "31898           0.0   7.225146  10717  0.143853  0.022400     -3.141593   \n",
       "31899           0.0   3.060886   8034  0.149462  0.025398     -3.141593   \n",
       "37821           0.0   8.330141   3470  0.114664  0.018964     -3.141593   \n",
       "37822           0.0   8.296052   2257  0.238676  0.041692     -3.141593   \n",
       "37823           0.0   9.557510   9507  0.308503  0.044157     -3.141593   \n",
       "131343          0.0   8.586030  18584  0.281316  0.038141     -3.141593   \n",
       "229011          0.0  16.692342   9504  0.142698  0.018955     -3.141593   \n",
       "229012          0.0  15.289849   7164  0.160584  0.021323     -3.141583   \n",
       "229022          0.0  11.916479   7786  0.281002  0.026128     -3.141593   \n",
       "229032          0.0   4.176556   8625  0.213572  0.029240     -3.141593   \n",
       "229041          0.0  19.419672   6151  0.391094  0.029653     -3.141593   \n",
       "\n",
       "                                               \n",
       "               max  count      mean    median  \n",
       "idcollar                                       \n",
       "31898     3.141593  10715 -0.020977 -0.033245  \n",
       "31899     3.141178   8032  0.037870  0.047588  \n",
       "37821     3.139853   3468 -0.002083 -0.038864  \n",
       "37822     3.140194   2255  0.000239  0.047619  \n",
       "37823     3.140023   9505 -0.020415 -0.025846  \n",
       "131343    3.141294  18582  0.035077  0.040854  \n",
       "229011    3.141526   9502 -0.015796 -0.021074  \n",
       "229012    3.141530   7162  0.001342 -0.007033  \n",
       "229022    3.141533   7784  0.015167 -0.001848  \n",
       "229032    3.141446   8623 -0.021255 -0.036740  \n",
       "229041    3.141226   6149  0.032170  0.040875  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['idcollar'])[['stepLength', 'turning_angle']].agg(['min','max','count', 'mean', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9655dd4b-4640-44e9-b036-19d36a058e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adjusted_tracking_data(file_path, id_pair, output_path):\n",
    "    \"\"\"\n",
    "    Extracts the tracking data within the adjusted shared timeline.\n",
    "    - Keeps three weeks before the shared start time for early starters (if possible).\n",
    "    - Keeps three weeks after the shared end time for late finishers (if possible).\n",
    "    - If the time difference is less than three weeks, keeps the original data.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert time column to datetime format\n",
    "    df['Time_LMT'] = pd.to_datetime(df['Time_LMT'])\n",
    "\n",
    "    # Filter for the selected individuals\n",
    "    df_pair = df[df['idcollar'].isin(id_pair)].copy()\n",
    "\n",
    "    # Find the earliest and latest tracking times for each individual\n",
    "    earliest_times = df_pair.groupby('idcollar')['Time_LMT'].min()\n",
    "    latest_times = df_pair.groupby('idcollar')['Time_LMT'].max()\n",
    "\n",
    "    # Determine the shared start and end times\n",
    "    shared_earliest_time = earliest_times.max()\n",
    "    shared_latest_time = latest_times.min()\n",
    "\n",
    "    # Apply a three-week buffer for early starters\n",
    "    adjusted_start_times = {}\n",
    "    for id in id_pair:\n",
    "        individual_start = earliest_times[id]\n",
    "        if (shared_earliest_time - individual_start) > pd.Timedelta(weeks=0):\n",
    "            adjusted_start_times[id] = shared_earliest_time - pd.Timedelta(weeks=0)\n",
    "        else:\n",
    "            adjusted_start_times[id] = individual_start  # Keep original if within three weeks\n",
    "\n",
    "    # Apply a three-week buffer for late finishers\n",
    "    adjusted_end_times = {}\n",
    "    for id in id_pair:\n",
    "        individual_end = latest_times[id]\n",
    "        if (individual_end - shared_latest_time) > pd.Timedelta(weeks=0):\n",
    "            adjusted_end_times[id] = shared_latest_time + pd.Timedelta(weeks=0)\n",
    "        else:\n",
    "            adjusted_end_times[id] = individual_end  # Keep original if within three weeks\n",
    "\n",
    "    # Extract rows within the adjusted timeline for each individual\n",
    "    df_filtered = pd.concat([\n",
    "        df_pair[(df_pair['idcollar'] == id) &\n",
    "                (df_pair['Time_LMT'] >= adjusted_start_times[id]) &\n",
    "                (df_pair['Time_LMT'] <= adjusted_end_times[id])]\n",
    "        for id in id_pair\n",
    "    ])\n",
    "\n",
    "    # Sort by time for proper sequence\n",
    "    df_filtered = df_filtered.sort_values(by=['Time_LMT'])\n",
    "\n",
    "    # Save the extracted dataset\n",
    "    df_filtered[['idcollar', 'Time_LMT', 'proj_lon', 'proj_lat', 'slope', 'sb', 'bt', 'gr']].to_csv(\n",
    "        output_path, index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Extracted tracking data saved to {output_path}\")\n",
    "\n",
    "    return df_filtered  # Return the filtered DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78600d0-f258-4eb4-99d6-72bc7b741a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_step_length_distribution(df, id_pair):\n",
    "    \"\"\"\n",
    "    Analyzes the step length distribution for a given pair of animals.\n",
    "    Fits multiple probability distributions and selects the best one.\n",
    "\n",
    "    \"\"\"\n",
    "    # Filter dataset for the selected individuals\n",
    "    df_pair = df[df['idcollar'].isin(id_pair)]\n",
    "\n",
    "    # Define distributions to test\n",
    "    distributions = [stats.gamma, stats.lognorm, stats.expon, stats.weibull_min]\n",
    "\n",
    "    # Store results\n",
    "    best_fits = {}\n",
    "\n",
    "    # Plot step length distributions and fit different distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i, id in enumerate(id_pair):\n",
    "        step_lengths = df_pair[df_pair['idcollar'] == id]['stepLength'].dropna()\n",
    "\n",
    "        # Histogram of step lengths\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.hist(step_lengths, bins=30, density=True, alpha=0.6, color='gray', label='Observed')\n",
    "\n",
    "        # Fit each distribution and compute log-likelihood\n",
    "        best_fit = None\n",
    "        best_ll = -np.inf\n",
    "        best_params = None\n",
    "\n",
    "        for dist in distributions:\n",
    "            params = dist.fit(step_lengths)  # Fit the distribution\n",
    "            ll = np.sum(dist.logpdf(step_lengths, *params))  # Compute log-likelihood\n",
    "\n",
    "            # Check if this is the best fit\n",
    "            if ll > best_ll:\n",
    "                best_ll = ll\n",
    "                best_fit = dist\n",
    "                best_params = params\n",
    "\n",
    "        # Store best fit\n",
    "        best_fits[id] = {\"distribution\": best_fit.name, \"parameters\": best_params}\n",
    "\n",
    "        # Plot best-fitting distribution\n",
    "        x = np.linspace(min(step_lengths), max(step_lengths), 100)\n",
    "        y = best_fit.pdf(x, *best_params)\n",
    "        plt.plot(x, y, label=f'Best fit: {best_fit.name}', linewidth=2)\n",
    "\n",
    "        plt.title(f'Step Length Distribution - {id}')\n",
    "        plt.xlabel('Step Length (km)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "\n",
    "        # Print best-fitting distribution parameters\n",
    "        print(f\"Best fit for {id}: {best_fit.name}\")\n",
    "        print(f\"Parameters: {best_params}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb960c53-5ba0-46db-be4e-799472b41823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mixture_wrapped_normal(angles):\n",
    "    \"\"\"\n",
    "    Fits a mixture of two wrapped normal distributions to capture bimodality.\n",
    "\n",
    "    Parameters:\n",
    "    - angles: np.array, observed turning angles\n",
    "\n",
    "    Returns:\n",
    "    - best_params: list of fitted wrapped normal parameters\n",
    "    \"\"\"\n",
    "    def wrapped_normal_mixture_neg_log_likelihood(params):\n",
    "        \"\"\"Negative log-likelihood function for a mixture of two wrapped normal distributions.\"\"\"\n",
    "        w, mu1, sigma1, mu2, sigma2 = params\n",
    "        w = np.clip(w, 0.01, 0.99)  # Ensure valid mixture weights\n",
    "        likelihood = (\n",
    "            w * stats.vonmises.pdf(angles, sigma1, loc=mu1) +\n",
    "            (1 - w) * stats.vonmises.pdf(angles, sigma2, loc=mu2)\n",
    "        )\n",
    "        return -np.sum(np.log(likelihood + 1e-9))  # Avoid log(0)\n",
    "\n",
    "    # Initial guesses: one component near -π, one near +π\n",
    "    initial_params = [0.5, -np.pi, 2, np.pi, 2]  # w, mu1, sigma1, mu2, sigma2\n",
    "    bounds = [(0.01, 0.99), (-np.pi, np.pi), (0.1, 10), (-np.pi, np.pi), (0.1, 10)]\n",
    "\n",
    "    # Optimize parameters using MLE\n",
    "    result = minimize(wrapped_normal_mixture_neg_log_likelihood, initial_params, bounds=bounds)\n",
    "    best_params = result.x\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2642333b-9d3c-4a5d-afac-31e16fa4a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_turning_angles(df, id_pair):\n",
    "    \"\"\"\n",
    "    Computes turning angles for a given pair of individuals.\n",
    "    \"\"\"\n",
    "    df_pair = df[df['idcollar'].isin(id_pair)].copy()\n",
    "\n",
    "    # Compute displacement vectors\n",
    "    df_pair['dx'] = df_pair.groupby('idcollar')['proj_lon'].diff()\n",
    "    df_pair['dy'] = df_pair.groupby('idcollar')['proj_lat'].diff()\n",
    "\n",
    "    # Compute movement directions (angles in radians)\n",
    "    df_pair['direction'] = np.arctan2(df_pair['dy'], df_pair['dx'])\n",
    "\n",
    "    # Compute turning angles (change in direction between consecutive steps)\n",
    "    df_pair['turning_angle'] = df_pair.groupby('idcollar')['direction'].diff()\n",
    "\n",
    "    # Normalize turning angles to range [-π, π]\n",
    "    df_pair['turning_angle'] = df_pair['turning_angle'].apply(lambda x: (x + np.pi) % (2 * np.pi) - np.pi)\n",
    "\n",
    "    return df_pair.dropna(subset=['turning_angle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "696255c8-ec1f-409d-8ef1-d279c469ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_turning_angle_distribution(df, id_pair):\n",
    "    \"\"\"\n",
    "    Analyzes the turning angle distribution for a given pair of animals.\n",
    "    Fits a Mixture of Two Wrapped Normal Distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing tracking data with 'turning_angle' column\n",
    "    - id_pair: list of two idcollar values (e.g., [37821, 229032])\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with the best-fitting model and parameters for each individual.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    best_fits = {}\n",
    "\n",
    "    for i, id in enumerate(id_pair):\n",
    "        angles = df[df['idcollar'] == id]['turning_angle'].dropna().values\n",
    "\n",
    "        # Histogram + KDE for visualization\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.hist(angles, bins=30, density=True, alpha=0.6, color='gray', label='Observed')\n",
    "        sns.kdeplot(angles, color='black', linewidth=1.5, label='KDE')\n",
    "\n",
    "        # Compute mean turn angles\n",
    "        mean_turn_angle = np.degrees(np.mean(angles))\n",
    "        \n",
    "        # Fit Mixture of Wrapped Normal Distributions\n",
    "        wrapped_normal_params = fit_mixture_wrapped_normal(angles)\n",
    "\n",
    "        # Compute persist_dir (small turns < 10°)\n",
    "        threshold = np.radians(10)  # Convert 10 degrees to radians\n",
    "        persist_dir = np.mean(np.abs(angles) < threshold)\n",
    "\n",
    "        # Compute std_persist_turns from only small turning angles\n",
    "        persist_turn_angles = angles[np.abs(angles) < threshold]\n",
    "        std_persist_turns = np.degrees(np.std(persist_turn_angles))\n",
    "\n",
    "        # Store best fit\n",
    "        best_fits[id] = {\"distribution\": \"Mixture of Wrapped Normals\", \n",
    "                         \"parameters\": wrapped_normal_params,\n",
    "                         \"persist_dir\": persist_dir,\n",
    "                         \"std_persist_turns\": std_persist_turns,\n",
    "                         \"mean_turn_angle\": mean_turn_angle\n",
    "                        }\n",
    "\n",
    "        # Plot best-fitting model\n",
    "        x = np.linspace(-np.pi, np.pi, 100)\n",
    "        w, mu1, sigma1, mu2, sigma2 = wrapped_normal_params\n",
    "        y = (\n",
    "            w * stats.vonmises.pdf(x, sigma1, loc=mu1) +\n",
    "            (1 - w) * stats.vonmises.pdf(x, sigma2, loc=mu2)\n",
    "        )\n",
    "\n",
    "        plt.plot(x, y, label=f'Best fit: Mixture of Wrapped Normals', linewidth=2)\n",
    "        plt.title(f'Turning Angle Distribution - {id}')\n",
    "        plt.xlabel('Turning Angle (radians)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "\n",
    "        # Print best-fitting distribution parameters\n",
    "        print(f\"Best fit for {id}: Mixture of Wrapped Normals\")\n",
    "        print(f\"Parameters: {wrapped_normal_params}\")\n",
    "        print(f\"Persist_dir for {id}: {persist_dir:.3f}, Std_persist_turns: {std_persist_turns:.3f}, Mean Turn Angle: {mean_turn_angle:.4f} degrees\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a71f81-461e-4980-9dda-037f6c06e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_slope_distribution(df, id_pair):\n",
    "    \"\"\"\n",
    "    Computes and plots the best-fit probability distributions for slope.\n",
    "    \"\"\"\n",
    "    distributions = [stats.chi2, stats.gamma, stats.lognorm]\n",
    "    best_fits = {}\n",
    "\n",
    "    for id in id_pair:\n",
    "        print(f\"\\n--- Analyzing Slope Distribution for {id} ---\")\n",
    "        best_fits[id] = {}\n",
    "\n",
    "        values = df[df[\"idcollar\"] == id][\"slope\"].dropna().values\n",
    "\n",
    "        if len(values) == 0:\n",
    "            print(f\"Warning: No slope values found for {id}'s home range.\")\n",
    "            continue\n",
    "\n",
    "        # Fit multiple distributions and select the best\n",
    "        best_fit = None\n",
    "        best_ll = -np.inf\n",
    "        best_params = None\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values, bins=30, density=True, alpha=0.6, color='gray', label='Observed')\n",
    "\n",
    "        for dist in distributions:\n",
    "            try:\n",
    "                params = dist.fit(values)\n",
    "                ll = np.sum(dist.logpdf(values, *params))\n",
    "\n",
    "                if ll > best_ll:\n",
    "                    best_ll = ll\n",
    "                    best_fit = dist\n",
    "                    best_params = params\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not fit {dist.name} for slope in {id}: {e}\")\n",
    "\n",
    "        # Store best fit\n",
    "        if best_fit:\n",
    "            best_fits[id][\"slope\"] = {\"distribution\": best_fit.name, \"parameters\": best_params}\n",
    "\n",
    "            # Plot best-fitting distribution\n",
    "            x = np.linspace(min(values), max(values), 100)\n",
    "            y = best_fit.pdf(x, *best_params)\n",
    "            plt.plot(x, y, label=f'Best fit: {best_fit.name}', linewidth=2)\n",
    "\n",
    "        plt.title(f\"Slope Distribution - {id}\")\n",
    "        plt.xlabel(\"Slope Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Print best-fitting distribution parameters\n",
    "        print(f\"Best fit for slope ({id}): {best_fit.name}\")\n",
    "        print(f\"Parameters: {best_params}\")\n",
    "\n",
    "    return best_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d40a46-4566-4585-9a7d-2801531d9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_fit_prey_distribution(df, id_pair, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Checks the prey occupancy distribution and fits probability distributions for included prey types.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing GPS tracking data\n",
    "    - id_pair: List of individual IDs (e.g., [37821, 229032])\n",
    "    - threshold: Minimum occupancy value required for inclusion\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with the best-fitting distribution and parameters for each included prey type per individual.\n",
    "    \"\"\"\n",
    "    # Define candidate distributions\n",
    "    distributions = [stats.gamma, stats.lognorm, stats.beta, stats.weibull_min]\n",
    "\n",
    "    best_fits = {}\n",
    "\n",
    "    for id in id_pair:\n",
    "        print(f\"\\n--- Checking and Fitting Prey Distribution for {id} ---\")\n",
    "        best_fits[id] = {}\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        for i, prey in enumerate([\"sb\", \"bt\", \"gr\"]):  # Include \"gr\" if relevant\n",
    "            values = df[df[\"idcollar\"] == id][prey].dropna()\n",
    "\n",
    "            # Plot histogram\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plt.hist(values, bins=30, color='gray', alpha=0.7, density=True)\n",
    "            plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=\"Threshold (0.6)\")\n",
    "            plt.xlabel(f\"{prey.capitalize()} Occupancy\")\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.title(f\"{prey.capitalize()} Occupancy Distribution - {id}\")\n",
    "            plt.legend()\n",
    "\n",
    "            # # Check if majority of values are below threshold\n",
    "            # below_threshold = np.sum(values < threshold) / len(values)\n",
    "            # if below_threshold > 0.5:  # If more than 50% of values are below 0.6, exclude this prey\n",
    "            #     print(f\"Excluding {prey} for {id} (majority occupancy < {threshold})\")\n",
    "            #     continue  # Skip fitting the distribution\n",
    "\n",
    "            # print(f\"Including {prey} for {id} (sufficient occupancy ≥ {threshold})\")\n",
    "\n",
    "            # Fit the best distribution\n",
    "            best_fit = None\n",
    "            best_ll = -np.inf\n",
    "            best_params = None\n",
    "\n",
    "            for dist in distributions:\n",
    "                try:\n",
    "                    params = dist.fit(values)\n",
    "                    ll = np.sum(dist.logpdf(values, *params))\n",
    "\n",
    "                    if ll > best_ll:\n",
    "                        best_ll = ll\n",
    "                        best_fit = dist\n",
    "                        best_params = params\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not fit {dist.name} for {prey} in {id}: {e}\")\n",
    "\n",
    "            # Store best fit\n",
    "            if best_fit:\n",
    "                best_fits[id][prey] = {\"distribution\": best_fit.name, \"parameters\": best_params}\n",
    "\n",
    "                # Plot best-fitting distribution\n",
    "                x = np.linspace(min(values), max(values), 100)\n",
    "                y = best_fit.pdf(x, *best_params)\n",
    "                plt.plot(x, y, label=f'Best fit: {best_fit.name}', linewidth=2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b434329-48dd-491b-8088-ad436de1dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pair = [131343, 37821]  # IDs of tiger and leopard\n",
    "file_path = \"../../data/tiger_leopard_env_tg.csv\"\n",
    "output_path = f\"../../data/adjusted_timeline_{id_pair[0]}_{id_pair[1]}.csv\"\n",
    "\n",
    "df_extracted = extract_adjusted_tracking_data(file_path, id_pair, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e61315e-2c2b-49fb-8f46-b524d3ac0fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Time_LMT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idcollar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>2019-10-09 10:00:00</td>\n",
       "      <td>2020-03-28 03:00:00</td>\n",
       "      <td>3470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131343</th>\n",
       "      <td>2019-10-09 10:00:00</td>\n",
       "      <td>2020-03-28 03:00:00</td>\n",
       "      <td>3913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time_LMT                          \n",
       "                         min                 max count\n",
       "idcollar                                              \n",
       "37821    2019-10-09 10:00:00 2020-03-28 03:00:00  3470\n",
       "131343   2019-10-09 10:00:00 2020-03-28 03:00:00  3913"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extracted.groupby(['idcollar'])[['Time_LMT']].agg(['min','max','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5ec16-3f08-40ac-9bcd-544259ab6dbc",
   "metadata": {},
   "source": [
    "# Prepare Combined Multi-context Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ae8ad0-ef8f-4aaa-ab67-063810100c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import MultiPoint, mapping\n",
    "import scipy.stats as stats\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "def clip_and_combine_probability(df, raster_paths, output_folder, id_pair, slope_fits, prey_fits, ssf_weights_dict):\n",
    "    \"\"\"\n",
    "    Clips required raster layers to MCP, aligns them, converts to probability, applies weighted sum using individual SSF coefficients, \n",
    "    normalizes, saves the final combined raster, and returns the combined probability arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing GPS tracking data with 'proj_lon' and 'proj_lat'\n",
    "    - raster_paths: Dictionary with raster file paths (keys include \"slope\", and any prey keys like \"bt\", \"gr\", etc.)\n",
    "    - output_folder: Path to save clipped and combined rasters\n",
    "    - id_pair: List of individual IDs (e.g., [37821, 229032])\n",
    "    - slope_fits: Dictionary with best-fit slope distributions\n",
    "    - prey_fits: Dictionary with best-fit prey distributions\n",
    "    - ssf_weights_dict: Dictionary of SSF weights for each individual\n",
    "\n",
    "    Returns:\n",
    "    - combined_probs: Dictionary mapping each individual ID to its combined probability array.\n",
    "    \"\"\" \n",
    "\n",
    "    combined_probs = {}\n",
    "    prob_means = {}\n",
    "    combined_masks = {}\n",
    "    \n",
    "    for id in id_pair:\n",
    "        print(f\"\\nProcessing individual {id}...\")\n",
    "\n",
    "        # Get the SSF weights for this individual\n",
    "        if id not in ssf_weights_dict:\n",
    "            print(f\"⚠ Warning: No SSF weights found for {id}. Skipping...\")\n",
    "            continue\n",
    "        ssf_weights = ssf_weights_dict[id]\n",
    "\n",
    "        # Filter only relevant environmental factors (slope or prey layers)\n",
    "        included_layers = [layer for layer in ssf_weights if layer in raster_paths]\n",
    "\n",
    "        if not included_layers:\n",
    "            print(f\"⚠ Skipping {id}: No relevant environmental factors (slope or prey layers).\")\n",
    "            continue  # Skip this individual\n",
    "\n",
    "        # # Get included environmental layers\n",
    "        # included_layers = [\"slope\"]  # Slope is always included\n",
    "        # included_layers += list(prey_fits.get(id, {}).keys())  # Add only included prey\n",
    "\n",
    "        # Create MCP (Minimum Convex Polygon)\n",
    "        subset = df[df[\"idcollar\"] == id]\n",
    "        points = [tuple(xy) for xy in subset[[\"proj_lon\", \"proj_lat\"]].values]\n",
    "        mcp = MultiPoint(points).convex_hull\n",
    "        mcp_gdf = gpd.GeoDataFrame(geometry=[mcp], crs=\"EPSG:32647\")  # UTM Zone 47N\n",
    "\n",
    "        clipped_rasters = {}\n",
    "\n",
    "        for layer in included_layers:\n",
    "            raster_path = raster_paths[layer]\n",
    "\n",
    "            with rasterio.open(raster_path) as src:\n",
    "                # Clip the raster to MCP\n",
    "                clipped_image, clipped_transform = mask(src, mcp_gdf.geometry, crop=True)\n",
    "\n",
    "                # Ensure correct raster shape\n",
    "                if len(clipped_image.shape) == 3 and clipped_image.shape[0] == 1:\n",
    "                    clipped_image = clipped_image[0]  # Remove singleton band dimension\n",
    "\n",
    "                # Save metadata\n",
    "                clipped_meta = src.meta.copy()\n",
    "                clipped_meta.update({\n",
    "                    \"height\": clipped_image.shape[0],  # Updated for correct shape\n",
    "                    \"width\": clipped_image.shape[1],\n",
    "                    \"transform\": clipped_transform\n",
    "                })\n",
    "\n",
    "                # Save clipped raster\n",
    "                clipped_raster_path = f\"{output_folder}/{layer}_clipped_MCP_{id}.tif\"\n",
    "                with rasterio.open(clipped_raster_path, \"w\", **clipped_meta) as dst:\n",
    "                    dst.write(clipped_image, 1)\n",
    "\n",
    "                print(f\"Clipped {layer} raster (MCP) saved for {id}: {clipped_raster_path}\")\n",
    "\n",
    "                # Store clipped raster for alignment\n",
    "                clipped_rasters[layer] = clipped_image\n",
    "\n",
    "        # rasterize the MCP to a hard mask\n",
    "        H, W = next(iter(clipped_rasters.values())).shape\n",
    "        hull_mask = rasterize(\n",
    "            [(mapping(mcp), 1)],\n",
    "            out_shape=(H, W),\n",
    "            transform=clipped_transform,\n",
    "            fill=0,\n",
    "            all_touched=True,\n",
    "            dtype=\"uint8\"\n",
    "        )\n",
    "        combined_masks[id] = hull_mask\n",
    "\n",
    "        # Step 1: Find the Smallest Common Shape (Height & Width)\n",
    "        min_height = min(image.shape[0] for image in clipped_rasters.values())\n",
    "        min_width = min(image.shape[1] for image in clipped_rasters.values())\n",
    "\n",
    "        # Step 2: Resize All Rasters to the Smallest Common Shape\n",
    "        aligned_rasters = {layer: image[:min_height, :min_width] for layer, image in clipped_rasters.items()}\n",
    "\n",
    "        # Step 3: Convert Cropped Rasters to Probability using SSF Weights\n",
    "        combined_prob = np.zeros((min_height, min_width), dtype=np.float32)\n",
    "\n",
    "        for layer, image in aligned_rasters.items():\n",
    "            if layer not in ssf_weights:\n",
    "                continue  # Skip layers without SSF weights\n",
    "\n",
    "            # # Clip extreme values\n",
    "            # image = np.clip(image, 0, np.percentile(image, 99))\n",
    "\n",
    "            # Handle extreme values by clipping within a reasonable range\n",
    "            if layer == \"slope\":\n",
    "                if id in slope_fits and \"slope\" in slope_fits[id]:  # Ensure slope exists in slope_fits\n",
    "                    dist_params = slope_fits[id][\"slope\"][\"parameters\"]\n",
    "                    image = np.clip(image, 0, np.percentile(image, 99))  # Clip outliers\n",
    "                    prob_layer = stats.lognorm.pdf(image, *dist_params)\n",
    "                else:\n",
    "                    print(f\"⚠ Warning: No slope fit found for {id}, skipping slope layer.\")\n",
    "                    continue\n",
    "        \n",
    "            else:  # For prey layers (e.g., bt, gr)\n",
    "                if id in prey_fits and layer in prey_fits[id]:  # Ensure layer exists in prey_fits\n",
    "                    dist_params = prey_fits[id][layer][\"parameters\"]\n",
    "                    image = np.clip(image, 0, np.percentile(image, 99))  # Clip extreme values\n",
    "                    prob_layer = stats.beta.pdf(image, *dist_params)\n",
    "                else:\n",
    "                    print(f\"⚠ Warning: No prey fit found for {id} - {layer}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "            # Fix: Remove invalid values (NaNs, negative values)\n",
    "            prob_layer[np.isnan(prob_layer)] = 0\n",
    "            prob_layer[prob_layer < 0] = 0\n",
    "\n",
    "            # Fix: Normalize each layer before applying weights\n",
    "            if np.nanmax(prob_layer) > 0:\n",
    "                prob_layer = (prob_layer - np.nanmin(prob_layer)) / (np.nanmax(prob_layer) - np.nanmin(prob_layer))\n",
    "            else:\n",
    "                prob_layer[:] = 0  # If no valid values, set to zero\n",
    "\n",
    "            # Fix: Apply log transformation to prevent extreme small values\n",
    "            prob_layer = np.log1p(prob_layer)\n",
    "\n",
    "            # Apply weight from SSF\n",
    "            combined_prob += ssf_weights[layer] * prob_layer\n",
    "\n",
    "        # Normalize final combined probability raster to 0-1\n",
    "        if np.nanmax(combined_prob) > 0:\n",
    "            combined_prob = (combined_prob - np.nanmin(combined_prob)) / (np.nanmax(combined_prob) - np.nanmin(combined_prob))\n",
    "        else:\n",
    "            combined_prob[:] = 0  # If no valid values, set to zero\n",
    "\n",
    "        # Ensure no negative values\n",
    "        combined_prob[combined_prob < 0] = 0\n",
    "\n",
    "        # Ensure correct shape\n",
    "        combined_prob = np.squeeze(combined_prob)\n",
    "        if combined_prob.ndim == 2:\n",
    "            combined_prob = combined_prob[np.newaxis, :, :]\n",
    "\n",
    "        print(\"Final combined_prob shape:\", combined_prob.shape)\n",
    "\n",
    "        # Compute Mean Probability\n",
    "        prob_mean = np.nanmean(combined_prob) if np.any(~np.isnan(combined_prob)) else 0\n",
    "        prob_means[id] = prob_mean\n",
    "        print(f\"Mean probability (prob_mean) for {id}: {prob_mean:.3f}\")\n",
    "\n",
    "        # Save the final raster\n",
    "        new_meta = clipped_meta.copy()\n",
    "        new_meta.update({\n",
    "            \"height\": combined_prob.shape[1],\n",
    "            \"width\": combined_prob.shape[2],\n",
    "            \"count\": 1\n",
    "        })\n",
    "\n",
    "        combined_output_path = f\"{output_folder}/combined_probability_MCP_{id}.tif\"\n",
    "        with rasterio.open(combined_output_path, \"w\", **new_meta) as dst:\n",
    "            dst.write(np.nan_to_num(combined_prob, nan=0).astype(rasterio.float32))\n",
    "\n",
    "        print(f\"Final combined probability raster saved for {id}: {combined_output_path}\")\n",
    "\n",
    "        combined_probs[id] = combined_prob\n",
    "\n",
    "    return combined_probs, prob_means, combined_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d692bb-0c05-4159-8077-48edbc4bb80c",
   "metadata": {},
   "source": [
    "# CRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32d6e8d8-b832-42c0-89be-b8d51a9f12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import rasterio\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec29385-06d9-49fe-9523-79064a211efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_angle(angle):\n",
    "    \"\"\"\n",
    "    Normalize any angle (in degrees) into the [0, 360) range.\n",
    "    \"\"\"\n",
    "    return angle % 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4103b8f2-0f9f-484e-9959-28524235a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_angle_distance_move(new_dir, dist):\n",
    "    \"\"\"\n",
    "    Given a direction (in degrees) and a distance,\n",
    "    compute the pixel offset (delta_x, delta_y) using a quadrant‐based method.\n",
    "    Assumes a coordinate system where x increases to the right and y increases downward.\n",
    "    \"\"\"\n",
    "    new_dir = test_angle(new_dir)\n",
    "    if 0 <= new_dir <= 90:\n",
    "        delta_x = round(dist * sin(radians(new_dir)))\n",
    "        delta_y = -round(dist * cos(radians(new_dir)))\n",
    "    elif 90 < new_dir <= 180:\n",
    "        delta_x = round(dist * sin(radians(180 - new_dir)))\n",
    "        delta_y = round(dist * cos(radians(180 - new_dir)))\n",
    "    elif 180 < new_dir <= 270:\n",
    "        delta_x = -round(dist * sin(radians(new_dir - 180)))\n",
    "        delta_y = round(dist * cos(radians(new_dir - 180)))\n",
    "    elif 270 < new_dir < 360:\n",
    "        delta_x = -round(dist * sin(radians(360 - new_dir)))\n",
    "        delta_y = -round(dist * cos(radians(360 - new_dir)))\n",
    "    else:\n",
    "        delta_x, delta_y = 0, 0\n",
    "    return (delta_x, delta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1fcc86a-e637-499c-9cda-678776bdf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell_boundary_matrix():\n",
    "    \"\"\"\n",
    "    Creates a point_list representing the eight adjacent cells.\n",
    "    Two extra points (first two again) are appended to allow wrap-around indexing.\n",
    "    \"\"\"\n",
    "    pl = point_list()\n",
    "    # Define the eight neighbors (clockwise starting from top-left)\n",
    "    neighbors = [(-1, -1), (0, -1), (1, -1),\n",
    "                 (1,  0), (1,  1), (0,  1),\n",
    "                 (-1, 1), (-1, 0)]\n",
    "    for pt in neighbors:\n",
    "        pl.append_point(Point(pt[0], pt[1]))\n",
    "    # Append the first two points again for easy wrap-around indexing.\n",
    "    for pt in neighbors[:2]:\n",
    "        pl.append_point(Point(pt[0], pt[1]))\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b947ef0-2d36-4731-94f9-7e4d7ee7ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    \"\"\"\n",
    "    A simple Point class.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def get_X(self):\n",
    "        return self.x\n",
    "    def get_Y(self):\n",
    "        return self.y\n",
    "    def get_coord(self):\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e8c0c4c-38ab-4c3f-bf10-445bb22b50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class point_list():\n",
    "    \"\"\"\n",
    "    A custom list to hold Point objects and provide convenience methods.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "    def get_length(self):\n",
    "        return len(self.points)\n",
    "    def get_Xs(self):\n",
    "        return [pt.get_X() for pt in self.points]\n",
    "    def get_Ys(self):\n",
    "        return [pt.get_Y() for pt in self.points]\n",
    "    def append_point(self, pt):\n",
    "        self.points.append(pt)\n",
    "    def get_point(self, n):\n",
    "        return self.points[n]\n",
    "    def plot_point_list_onRaster(self, myArray):\n",
    "        plt.plot(self.get_Xs(), self.get_Ys(), 'ro-')\n",
    "        plt.plot(self.get_point(0).get_X(), self.get_point(0).get_Y(), 'gs', label=\"Start\")\n",
    "        plt.plot(self.get_point(self.get_length()-1).get_X(), self.get_point(self.get_length()-1).get_Y(), 'ys', label=\"End\")\n",
    "        imgplot = plt.imshow(myArray, vmin=0, vmax=1024)\n",
    "        imgplot.set_cmap('spectral')\n",
    "        plt.title(\"Track on Raster\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def plot_point_list(self):\n",
    "        plt.plot(self.get_Xs(), self.get_Ys(), 'ro-')\n",
    "        plt.plot(self.get_point(0).get_X(), self.get_point(0).get_Y(), 'gs', label=\"Start\")\n",
    "        plt.plot(self.get_point(self.get_length()-1).get_X(), self.get_point(self.get_length()-1).get_Y(), 'ys', label=\"End\")\n",
    "        plt.title(\"Track\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d0d3b63-721f-423c-9f92-6dc30a1268d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads a raster and converts it to a numpy array\n",
    "def raster_to_numpy_array(rasterFile):\n",
    "    # open raster dataset\n",
    "    dem = gdal.Open(rasterFile)\n",
    "\n",
    "    print ('Driver: ', dem.GetDriver().ShortName,'/', dem.GetDriver().LongName,)\n",
    "    print ('Size is ',dem.RasterXSize,'x',dem.RasterYSize,'x',dem.RasterCount)\n",
    "    print ('Projection is ',dem.GetProjection())\n",
    "    geotransform = dem.GetGeoTransform()\n",
    "    if not geotransform is None:\n",
    "        print ('Origin = (',geotransform[0], ',',geotransform[3],')')\n",
    "        print ('Pixel Size = (',geotransform[1], ',',geotransform[5],')')\n",
    "    myArray = np.array(dem.GetRasterBand(1).ReadAsArray())\n",
    "\n",
    "    return myArray, geotransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd58362-405d-4a99-993b-97d5a8d02c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create a GeoTiff raster with specification of originalInputRaster\n",
    "# and saves a numpy array to that GeoTiff\n",
    "def numpy_array_to_raster(myArray,outputFile, originalInputRaster):\n",
    "    inRaster = gdal.Open(originalInputRaster)\n",
    "\n",
    "    # get parameters\n",
    "    geotransform = inRaster.GetGeoTransform()\n",
    "    spatialreference = inRaster.GetProjection()\n",
    "    ncol = inRaster.RasterXSize\n",
    "    nrow = inRaster.RasterYSize\n",
    "    nband = 1\n",
    "    # create dataset for output\n",
    "    fmt = 'GTiff'\n",
    "    driver = gdal.GetDriverByName(fmt)\n",
    "    dst_dataset = driver.Create(outputFile, ncol, nrow, nband, gdal.GDT_Float32)\n",
    "    dst_dataset.SetGeoTransform(geotransform)\n",
    "    dst_dataset.SetProjection(spatialreference)\n",
    "    dst_dataset.GetRasterBand(1).WriteArray(myArray)\n",
    "    dst_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e15de2b9-6cd2-4466-b2eb-bbe9e708a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_slice(n_dir):\n",
    "       # ******************  select three closest pixels in n_dir ***********************\n",
    "        num_slices = 8\n",
    "        seg_size = 360 / num_slices\n",
    "        range_1 = - seg_size / 2.0 + 360\n",
    "        range_2 = (range_1 + seg_size) - 360\n",
    "        slice_ind = 0\n",
    "        if (n_dir >= range_1) or (n_dir <= range_2):\n",
    "            slice_ind = 1\n",
    "        # print(\"slice \",slice)\n",
    "        else:\n",
    "            # (find which segment the new_dir is in\n",
    "            # print(\" **** In else loop ****\")\n",
    "            for p in range(num_slices - 1):\n",
    "                range_1 += seg_size\n",
    "                range_2 += seg_size\n",
    "                # print(\"Range in for loop \",range_1,range_2)\n",
    "                if range_1 > 360:\n",
    "                    range_1 -= 360\n",
    "                # print(\"range 1 & 2\",range_1,range_2)\n",
    "                if range_1 < n_dir <= range_2:\n",
    "                    slice_ind = p + 2\n",
    "                    break\n",
    "        # print(\"slice, new direction\", slice,n_dir)\n",
    "        return slice_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ce9305-5391-40f6-9905-dd662994bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animating_track(x_coord,y_coord, back_raster):\n",
    "    [cols,rows]= back_raster.shape\n",
    "    x = len(x_coord)\n",
    "    # animation code\n",
    "    # plt.subplot(1,2,1)\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(xlim=(0, rows), ylim=(cols,0))\n",
    "    line, = ax.plot([], [], 'wo',linewidth=0)\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        return line,\n",
    "\n",
    "    def animate(i):\n",
    "        # line.set_data(i,i)\n",
    "        line.set_data(x_coord[1:i], y_coord[1:i])\n",
    "        return line,\n",
    "\n",
    "    # use x for number of points/frames\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=x, interval=50, blit=True)\n",
    "    imgplot = ax.imshow(back_raster, cmap=\"viridis\", vmin=0, vmax=np.nanmax(back_raster))\n",
    "    # imgplot.set_cmap('spectral')\n",
    "    plt.show()\n",
    "    print(\"Track animation done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05584701-3fd1-4279-b9ad-90b72e749d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crw_combined(num_pts, persis_dir, mu, sigma, sigma_1, \n",
    "                 myArray, cols, rows, envArray, hull_mask, mat_use, \n",
    "                 time_step, start_time, \n",
    "                 id_selected, best_step_length_fits, best_turning_angle_fits,\n",
    "                 max_tries_per_step = 50\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Simulated CRW that uses the combined probability raster (envArray) as context.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nInitializing CRW simulation...\")\n",
    "\n",
    "    # Get movement parameters for the selected individual\n",
    "    step_params = best_step_length_fits[id_selected][\"parameters\"]\n",
    "    turn_params = best_turning_angle_fits[id_selected][\"parameters\"]\n",
    "    mix_p, mu1, kappa1, mu2, kappa2 = turn_params\n",
    "    \n",
    "    # Choose a random starting point that is valid (within myArray range)\n",
    "    valid = False\n",
    "    while not valid:\n",
    "        new_x = np.random.randint(0, cols)\n",
    "        new_y = np.random.randint(0, rows)\n",
    "        if (0 <= new_x < cols and 0 <= new_y < rows and \n",
    "            0 <= myArray[new_y, new_x] <= 1 and \n",
    "            hull_mask[new_y, new_x] == 1 and\n",
    "            envArray[new_y, new_x] > 0):\n",
    "            valid = True\n",
    "     \n",
    "    x_coord = [new_x]\n",
    "    y_coord = [new_y]\n",
    "    mat_use[new_y, new_x] += 1\n",
    "    \n",
    "    # Set initial direction (random 0-360 degrees)\n",
    "    prev_dir = random.uniform(0, 360)\n",
    "\n",
    "    print(f\"Starting simulation at ({new_x}, {new_y})\")\n",
    "    print(\"Entering main simulation loop...\")\n",
    "\n",
    "    test_pts = 0  # count of simulated steps\n",
    "    \n",
    "    while test_pts <= num_pts:\n",
    "        test_pts += 1\n",
    "\n",
    "        if test_pts % 1000 == 0:  # Print every 1000 steps to track progress\n",
    "            print(f\"Step {test_pts}/{num_pts} - Current position: ({new_x}, {new_y})\")\n",
    "\n",
    "        retries = 0\n",
    "        success = False\n",
    "        while retries < max_tries_per_step and not success:\n",
    "            retries += 1\n",
    "            \n",
    "            # --- Determine new direction ---\n",
    "            if random.random() < persis_dir:\n",
    "                # new_dir = prev_dir + np.random.vonmises(mu2, sigma2)\n",
    "                new_dir = (prev_dir + np.random.normal(mu, sigma))\n",
    "            else:\n",
    "                # new_dir = prev_dir + np.random.vonmises(mu3, sigma3)\n",
    "                new_dir = (prev_dir + np.random.normal(mu, sigma_1))\n",
    "            new_dir = test_angle(new_dir)  # adjust to [0,360)\n",
    "            \n",
    "            # --- Sample a step length from the fitted lognorm distribution ---\n",
    "            # Using np.random.lognormal with parameters: mean and sigma of underlying normal.\n",
    "            step_km = np.random.lognormal(step_params[0], step_params[2]) # increase variation\n",
    "            step_m  = step_km * 1000\n",
    "            max_m   = 300 * time_step # 0.3 km/h\n",
    "            step_m  = min(step_m, max_m)            \n",
    "    \n",
    "            # # Limit max step length\n",
    "            pixel_size = 30\n",
    "            step_pixels = int(round(step_m / pixel_size))\n",
    "\n",
    "            # print(f\"Step {test_pts}: Step length = {step:.2f}m \")\n",
    "            \n",
    "            # Compute endpoint using the step length and new direction (as a vector)\n",
    "            delta_x, delta_y = compute_angle_distance_move(new_dir, step_pixels)\n",
    "            t_x = new_x + delta_x\n",
    "            t_y = new_y + delta_y\n",
    "        \n",
    "            # Check if endpoint is valid in DEM (myArray)\n",
    "            if (t_x < 0 or t_x >= cols or t_y < 0 or t_y >= rows or \n",
    "                myArray[t_y, t_x] < 0 or myArray[t_y, t_x] > 1 or  hull_mask[t_y, t_x] == 0 or\n",
    "                envArray[t_y, t_x] <= 0 or np.isnan(envArray[t_y, t_x])):\n",
    "                retries += 1\n",
    "                # print(f\"Step {test_pts}: Invalid position ({t_x}, {t_y}), retrying...\")\n",
    "                # If invalid, reinitialize direction randomly and retry this step\n",
    "                # **Limit retries per step to prevent infinite loops**\n",
    "                if retries > 100:\n",
    "                    # print(f\"    Too many retries at step {test_pts}, adjusting strategy.\")\n",
    "                    prev_dir = np.random.random() * 360  # Choose a random direction to escape\n",
    "                    retries = 0  # Reset retry count\n",
    "                success = False\n",
    "                continue  # Skip this step\n",
    "\n",
    "            # success >= accept\n",
    "            success = True\n",
    "            new_x, new_y = t_x, t_y\n",
    "            x_coord.append(new_x)\n",
    "            y_coord.append(new_y)\n",
    "            mat_use[new_y, new_x] += 1\n",
    "\n",
    "            # update direction\n",
    "            prev_dir = new_dir\n",
    "            retries = 0\n",
    "\n",
    "        if not success:\n",
    "            pass\n",
    "            \n",
    "        # End of one step: update current position to last new_x, new_y\n",
    "        # (They are already updated in the loop.)\n",
    "    print(\"CRW Done! number of points =\", len(x_coord))\n",
    "    return mat_use, x_coord, y_coord #, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "271c877c-37e6-4641-9f44-7599052c60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visit_matrix(visit_matrix, geotransform, title=\"Visitation Matrix\"):\n",
    "    \"\"\"Plots the visitation matrix.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(visit_matrix, cmap=\"hot\", interpolation=\"nearest\", origin=\"upper\")\n",
    "    plt.colorbar(label=\"Visit Count\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X Pixels\")\n",
    "    plt.ylabel(\"Y Pixels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccbde173-6e3b-469a-b4b5-098b25cc5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_observed_visit_matrix(df_indiv, geotransform, rows, cols):\n",
    "    \"\"\"Computes visit matrix from observed tracking data.\"\"\"\n",
    "    visit_matrix = np.zeros((rows, cols), dtype=np.int64)\n",
    "    \n",
    "    for _, row in df_indiv.iterrows():\n",
    "        x, y = row['proj_lon'], row['proj_lat']\n",
    "        col_idx = int((x - geotransform[0]) / geotransform[1])\n",
    "        row_idx = int((geotransform[3] - y) / abs(geotransform[5]))\n",
    "\n",
    "        # Skip out-of-bounds points\n",
    "        if 0 <= row_idx < rows and 0 <= col_idx < cols:\n",
    "            visit_matrix[row_idx, col_idx] += 1  \n",
    "        else:\n",
    "            print(f\"⚠ Skipping out-of-bounds point: ({x}, {y}) → (row={row_idx}, col={col_idx})\")\n",
    "            \n",
    "    return visit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a956de0-c769-470f-abde-1fa39e1cfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side_visitation_matrices(observed_matrix, simulated_matrix, M_C):\n",
    "    \"\"\"\n",
    "    Plots observed and simulated visit matrices side by side for comparison.\n",
    "    \n",
    "    - The highest 10% of values (95th percentile and above) are displayed as the same color (white).\n",
    "    - The simulated matrix is divided by M_C (number of Monte Carlo runs) to ensure comparability.\n",
    "    - A consistent color scale is applied to both matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize simulated matrix by dividing by M_C\n",
    "    simulated_matrix = simulated_matrix / M_C\n",
    "\n",
    "    # Extract nonzero values to compute color scale\n",
    "    observed_nonzero = observed_matrix[observed_matrix > 0]\n",
    "    simulated_nonzero = simulated_matrix[simulated_matrix > 0]\n",
    "\n",
    "    # Compute the 95th percentile to avoid extreme outliers dominating\n",
    "    observed_p95 = np.percentile(observed_nonzero, 95) if len(observed_nonzero) > 0 else 1\n",
    "    simulated_p95 = np.percentile(simulated_nonzero, 95) if len(simulated_nonzero) > 0 else 1\n",
    "\n",
    "    # Use the same color scale for both matrices based on the highest 95th percentile\n",
    "    color_max = max(observed_p95, simulated_p95, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Observed Visit Matrix with adjusted color scaling\n",
    "    im1 = axes[0].imshow(observed_matrix, cmap=\"hot\", interpolation=\"nearest\", origin=\"upper\",\n",
    "                         vmin=0, vmax=color_max)\n",
    "    axes[0].set_title(\"Observed Visitation Matrix\")\n",
    "    cbar1 = plt.colorbar(im1, ax=axes[0])\n",
    "    cbar1.set_label(\"Visit Count\")\n",
    "\n",
    "    # Simulated Visit Matrix with adjusted color scaling\n",
    "    im2 = axes[1].imshow(simulated_matrix, cmap=\"hot\", interpolation=\"nearest\", origin=\"upper\",\n",
    "                         vmin=0, vmax=color_max)\n",
    "    axes[1].set_title(\"Simulated Visitation Matrix\")\n",
    "    cbar2 = plt.colorbar(im2, ax=axes[1])\n",
    "    cbar2.set_label(\"Visit Count (Averaged)\")\n",
    "\n",
    "    # Labels\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(\"X Pixels\")\n",
    "        ax.set_ylabel(\"Y Pixels\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ab860eb-4320-4ec4-adfe-d8b6987860e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_and_test(observed_matrix, simulated_matrix, M_C):\n",
    "    \"\"\"\n",
    "    Plots a boxplot comparing observed and simulated visitation counts \n",
    "    and performs a statistical test (Mann-Whitney U test).\n",
    "    \"\"\"\n",
    "\n",
    "    # # Compute the scaling factor for the simulated data\n",
    "    # scaling_factor =  simulated_total_pts / observed_total_pts \n",
    "    # print(f\"Scaling factor: {scaling_factor:.4f}\")\n",
    "\n",
    "    # Scale the simulated visitation counts\n",
    "    scaled_simulated_visitation = simulated_matrix / M_C\n",
    "\n",
    "    # Flatten matrices to extract nonzero visitation counts\n",
    "    observed_counts = observed_matrix.flatten()\n",
    "    simulated_counts = scaled_simulated_visitation.flatten()\n",
    "\n",
    "    # Remove zero values (cells that were never visited)\n",
    "    observed_counts = observed_counts[observed_counts > 0]\n",
    "    simulated_counts = simulated_counts[simulated_counts > 0]\n",
    "\n",
    "    # Remove NaN values before performing statistical tests**\n",
    "    observed_counts = observed_counts[~np.isnan(observed_counts)]\n",
    "    simulated_counts = simulated_counts[~np.isnan(simulated_counts)]\n",
    "\n",
    "    # **Ensure observed and simulated counts have the same length without NaNs**\n",
    "    min_len = min(len(observed_counts), len(simulated_counts))\n",
    "    observed_counts = observed_counts[:min_len]  # Trim to shortest length\n",
    "    simulated_counts = simulated_counts[:min_len]  # Trim to shortest length\n",
    "\n",
    "    # Convert to DataFrame for easier boxplotting\n",
    "    df_boxplot = pd.DataFrame({\n",
    "        \"Observed\": observed_counts,\n",
    "        \"Simulated\": simulated_counts\n",
    "    })\n",
    "    print(f\"Observed count size: {len(observed_counts)}\")\n",
    "    print(f\"Simulated count size: {len(simulated_counts)}\")\n",
    "    print(\"Unique values in observed:\", np.unique(observed_counts))\n",
    "    print(\"Unique values in simulated:\", np.unique(simulated_counts))\n",
    "    \n",
    "    # Plot boxplot comparing visitation distributions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df_boxplot.boxplot(column=[\"Observed\", \"Simulated\"], showfliers=True)\n",
    "    # plt.yscale(\"log\")  # Log scale to handle skewness in visitation data\n",
    "    plt.ylabel(\"Visitation Count\")\n",
    "    plt.title(\"Comparison of Visitation Distributions (Observed vs. Simulated)\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Perform Mann-Whitney U Test ---\n",
    "    if len(observed_counts) > 0 and len(simulated_counts) > 0:\n",
    "        stat, p_value = mannwhitneyu(observed_counts, simulated_counts, alternative='two-sided')\n",
    "        print(f\"🔍 Mann-Whitney U test results:\")\n",
    "        print(f\"   U-statistic: {stat:.3f}\")\n",
    "        print(f\"   p-value: {p_value:.3e}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"   ➡ Significant difference detected between observed and simulated visitations (p < 0.05).\")\n",
    "        else:\n",
    "            print(\"   ➡ No significant difference detected (p ≥ 0.05).\")\n",
    "    else:\n",
    "        print(\"⚠ Skipping statistical test: One or both distributions are empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62e74b51-3cc9-46d5-b731-9f0fb3d79b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crw_monte_carlo_simulation(M_C, num_pts, persis_dir, mu, sigma, sigma_1, \n",
    "                                      myArray, cols, rows, envArray, hull_mask, mat_use, time_step, \n",
    "                                      start_time, id_selected, best_step_length_fits, best_turning_angle_fits):\n",
    "    \n",
    "    all_simulated_data = []  # Store all MC simulation trajectories\n",
    "    # For this example, we run one MC simulation; you can loop over M_C if needed.\n",
    "    for mc_round in range(1, M_C + 1):\n",
    "        print(f\"Beginning Monte Carlo iteration {mc_round}/{M_C}\")\n",
    "        current_mat_use, xc, yc = crw_combined(num_pts, persis_dir, mu, sigma, sigma_1, \n",
    "                                               myArray, cols, rows, envArray, hull_mask, mat_use, time_step,\n",
    "                                               start_time, id_selected, best_step_length_fits, best_turning_angle_fits)\n",
    "        # Accumulate visitation matrix across MC runs\n",
    "        mat_use += current_mat_use\n",
    "\n",
    "        if len(xc) > 1:\n",
    "            # total time for the entire track\n",
    "            total_duration_hours = num_pts * time_step\n",
    "            # We'll assign times linearly from start_time to start_time + total_duration_hours\n",
    "            time_list = [\n",
    "                start_time + pd.Timedelta(hours=(total_duration_hours * i/(len(xc)-1)))\n",
    "                for i in range(len(xc))\n",
    "            ]\n",
    "        else:\n",
    "            time_list = [start_time] * len(xc)\n",
    "            \n",
    "        # Store trajectory with MC round information\n",
    "        df_mc = pd.DataFrame({\n",
    "            \"idcollar\": [id_selected] * len(xc),\n",
    "            \"x\": xc,\n",
    "            \"y\": yc,\n",
    "            \"time\": time_list,\n",
    "            \"MC_round\": [mc_round] * len(xc)\n",
    "        })\n",
    "        \n",
    "        all_simulated_data.append(df_mc)\n",
    "\n",
    "    # Combine all Monte Carlo trajectories into a single DataFrame\n",
    "    df_all_mc = pd.concat(all_simulated_data, ignore_index=True)\n",
    "    \n",
    "    return mat_use, df_all_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b34cecc-b947-470f-a52d-2d36f46758ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_clipped_raster(df, clipped_raster_path, geotransform):\n",
    "    \"\"\"\n",
    "    Extracts environmental values from a clipped raster based on x, y pixel locations.\n",
    "    Also converts pixel indices to projected coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with x, y pixel indices\n",
    "    - clipped_raster_path: File path to the individual's clipped raster\n",
    "    - geotransform: Raster geotransform to convert pixel indices to projected coordinates\n",
    "\n",
    "    Returns:\n",
    "    - List of extracted values\n",
    "    - Lists of projected longitude and latitude\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(clipped_raster_path) as src:\n",
    "            raster_data = src.read(1)  # Read the first band\n",
    "            nodata_val = src.nodata  # NoData value\n",
    "\n",
    "            extracted_values = []\n",
    "            proj_lons = []\n",
    "            proj_lats = []\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                x_pix, y_pix = int(row[\"x\"]), int(row[\"y\"])  # Convert to integer pixel indices\n",
    "\n",
    "                if 0 <= x_pix < src.width and 0 <= y_pix < src.height:\n",
    "                    value = raster_data[y_pix, x_pix]\n",
    "\n",
    "                    # Handle NoData values\n",
    "                    if nodata_val is not None and value == nodata_val:\n",
    "                        value = np.nan\n",
    "\n",
    "                    extracted_values.append(value)\n",
    "\n",
    "                    # Convert pixel index to projected coordinates\n",
    "                    lon = geotransform[0] + x_pix * geotransform[1]\n",
    "                    lat = geotransform[3] + y_pix * geotransform[5]\n",
    "                    proj_lons.append(lon)\n",
    "                    proj_lats.append(lat)\n",
    "                else:\n",
    "                    extracted_values.append(np.nan)\n",
    "                    proj_lons.append(np.nan)\n",
    "                    proj_lats.append(np.nan)\n",
    "\n",
    "            return extracted_values, proj_lons, proj_lats\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error reading {clipped_raster_path}: {e}\")\n",
    "        return [np.nan] * len(df), [np.nan] * len(df), [np.nan] * len(df)  # Return NaNs if raster cannot be read\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf62ea-710d-405c-a83d-936456a61def",
   "metadata": {},
   "source": [
    "# Interaction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3840ac-9426-477b-8c62-191c5033aba0",
   "metadata": {},
   "source": [
    "# Reverse slicing for every 1000 hours (Tail to Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a555da24-235e-4ffe-8558-6335b3e480c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_step_diagnostics(df_slice, id_pair, slice_idx, win_start=None, win_end=None):\n",
    "    if win_start and win_end:\n",
    "        print(f\"\\n[Slice {slice_idx:04d}] window {win_start} → {win_end}\")\n",
    "    else:\n",
    "        print(f\"\\n[Slice {slice_idx:04d}] Time Interval Diagnostics:\")\n",
    "\n",
    "    for id_sel in id_pair:\n",
    "        df_indiv = df_slice[df_slice[\"idcollar\"] == id_sel].sort_values(\"time\")\n",
    "        df_indiv[\"time\"] = pd.to_datetime(df_indiv[\"time\"], errors=\"coerce\")\n",
    "        time_deltas = df_indiv[\"time\"].diff().dropna()\n",
    "        n_points = len(df_indiv)\n",
    "        if not time_deltas.empty:\n",
    "            median_step = time_deltas.median().total_seconds() / 60.0  # in minutes\n",
    "            print(f\"  ID {id_sel}: n = {n_points} | median = {median_step:.2f} min | \"\n",
    "                  f\"min = {time_deltas.min()} | max = {time_deltas.max()}\")\n",
    "        else:\n",
    "            print(f\"  ID {id_sel}: n = {n_points} | Not enough data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7834ae4-6e91-41be-b2ff-dc5a98c4bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(id_pair)\n",
    "file_path = \"../../data/tiger_leopard_env_tg.csv\"\n",
    "\n",
    "# 1) read & slice your *full* calibrated CSV\n",
    "df_full = extract_adjusted_tracking_data(file_path, id_pair, \n",
    "                                         f\"../../data/adjusted_timeline_{id_pair[0]}_{id_pair[1]}.csv\")\n",
    "df_full[\"Time_LMT\"] = pd.to_datetime(df_full[\"Time_LMT\"])\n",
    "df_full = df_full.rename(columns={\"Time_LMT\":\"time\"})\n",
    "\n",
    "# Convert 'time' to datetime format (this fixes the issue)\n",
    "df_full[\"time\"] = pd.to_datetime(df_full[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Now, apply the correct formatting (if needed)\n",
    "df_full[\"time\"] = df_full[\"time\"].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_full[\"time\"] = pd.to_datetime(df_full[\"time\"], errors=\"coerce\")\n",
    "df_full.head()\n",
    "\n",
    "\n",
    "# 1 000‑hour windows anchored at the end so every slice is full length\n",
    "overall_start = df_full[\"time\"].min()\n",
    "overall_end   = df_full[\"time\"].max()\n",
    "window        = pd.Timedelta(hours=1000)\n",
    "\n",
    "# build reverse windows ending at overall_end\n",
    "rev_windows = []\n",
    "cur_end = overall_end\n",
    "while True:\n",
    "    cur_start = cur_end - window\n",
    "    if cur_start < overall_start:\n",
    "        # head‐chunk shorter than 1000 h → drop\n",
    "        break\n",
    "    rev_windows.append((cur_start, cur_end))\n",
    "    cur_end = cur_start\n",
    "\n",
    "# reverse to chronological so slice000 is earliest\n",
    "windows = list(reversed(rev_windows))\n",
    "n_slices = len(windows)\n",
    "print(f\"Total slices of 1000 h each: {n_slices}\")\n",
    "\n",
    "# diagnostics\n",
    "for i, (start, end) in enumerate(windows):\n",
    "    s1 = df_full[(df_full[\"idcollar\"] == id_pair[0]) &\n",
    "                 (df_full[\"time\"] >= start) &\n",
    "                 (df_full[\"time\"] <  end)]\n",
    "    s2 = df_full[(df_full[\"idcollar\"] == id_pair[1]) &\n",
    "                 (df_full[\"time\"] >= start) &\n",
    "                 (df_full[\"time\"] <  end)]\n",
    "    df_slice = pd.concat([s1, s2], ignore_index=True)\n",
    "    print_time_step_diagnostics(df_slice, id_pair, i, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf2d7-8ae2-44b1-ad53-7980d402e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "raster_paths = {\n",
    "    \"slope\": \"D:/Tiger/Map/Environmental data/DEM/slope_wefcom_UTM47N.tif\",\n",
    "    \"sb\": \"D:/Tiger/Map/Environmental data/Prey/Occupancy Data/sb1kmPredictiveMap_UTM47N.tif\",\n",
    "    \"bt\": \"D:/Tiger/Map/Environmental data/Prey/Occupancy Data/bt1kmPredictiveMap_UTM47N.tif\",\n",
    "    \"gr\": \"D:/Tiger/Map/Environmental data/Prey/Occupancy Data/gr1kmPredictiveMap_UTM47N.tif\"\n",
    "}\n",
    "\n",
    "ssf_weights_dict = {\n",
    "    31898: {\"bt\": 0.4572, \"cos_ta\": -0.0475},\n",
    "    # 31899: {\"cos_ta\": -0.1140},\n",
    "    37821: {\"bt\": 0.3930, \"gr\": 0.4409, \"slope\": -0.1226, \"cos_ta\": -0.2147},\n",
    "    # 37822: {\"cos_ta\": 0.1180},\n",
    "    37823: {\"bt\": 0.4614, \"gr\": 0.4045, \"slope\": -0.2446, \"cos_ta\": 0.1035},\n",
    "    131343: {\"bt\": 0.7164, \"slope\": -0.0826},\n",
    "    229011: {\"slope\": -0.0543, \"cos_ta\": -0.1512},\n",
    "    229012: {\"bt\": 0.7511, \"cos_ta\": -0.1318, \"slope\": -0.0871},\n",
    "    229022: {\"slope\": -2291, \"cos_ta\": -0.0828},\n",
    "    229032: {\"bt\": 0.7476, \"cos_ta\": -0.0328, \"slope\": -0.1160},\n",
    "    229041: {\"bt\": 0.7659, \"slope\": -0.2764}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476fcc2-4a3f-4cbc-b9af-0139455c5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import workers_null_model\n",
    "\n",
    "id_pair = [131343, 37821]\n",
    "slice_output_base = Path(f\"../../simulated_trajectory/CRW_slice_RT_MC1000/by_slice_{id_pair[0]}_{id_pair[1]}_reverse_time\")\n",
    "file_path = \"../../data/tiger_leopard_env_tg.csv\"\n",
    "\n",
    "# 1) read & slice your *full* calibrated CSV\n",
    "df_full = extract_adjusted_tracking_data(file_path, id_pair, \n",
    "                                         f\"../../data/adjusted_timeline_{id_pair[0]}_{id_pair[1]}.csv\")\n",
    "df_full[\"Time_LMT\"] = pd.to_datetime(df_full[\"Time_LMT\"])\n",
    "df_full = df_full.rename(columns={\"Time_LMT\":\"time\"})\n",
    "\n",
    "# Convert 'time' to datetime format (this fixes the issue)\n",
    "df_full[\"time\"] = pd.to_datetime(df_full[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Now, apply the correct formatting (if needed)\n",
    "df_full[\"time\"] = df_full[\"time\"].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_full[\"time\"] = pd.to_datetime(df_full[\"time\"], errors=\"coerce\")\n",
    "df_full.head()\n",
    "\n",
    "\n",
    "# 1 000‑hour windows anchored at the end so every slice is full length\n",
    "overall_start = df_full[\"time\"].min()\n",
    "overall_end   = df_full[\"time\"].max()\n",
    "window        = pd.Timedelta(hours=1000)\n",
    "\n",
    "# build reverse windows ending at overall_end\n",
    "rev_windows = []\n",
    "cur_end = overall_end\n",
    "while True:\n",
    "    cur_start = cur_end - window\n",
    "    if cur_start < overall_start:\n",
    "        # head‐chunk shorter than 1000 h → drop\n",
    "        break\n",
    "    rev_windows.append((cur_start, cur_end))\n",
    "    cur_end = cur_start\n",
    "\n",
    "# reverse to chronological so slice000 is earliest\n",
    "windows = list(reversed(rev_windows))\n",
    "n_slices = len(windows)\n",
    "print(f\"Total slices of 1000 h each: {n_slices}\")\n",
    "\n",
    "# diagnostics\n",
    "for i, (start, end) in enumerate(windows):\n",
    "    s1 = df_full[(df_full[\"idcollar\"] == id_pair[0]) &\n",
    "                 (df_full[\"time\"] >= start) &\n",
    "                 (df_full[\"time\"] <  end)]\n",
    "    s2 = df_full[(df_full[\"idcollar\"] == id_pair[1]) &\n",
    "                 (df_full[\"time\"] >= start) &\n",
    "                 (df_full[\"time\"] <  end)]\n",
    "    df_slice = pd.concat([s1, s2], ignore_index=True)\n",
    "    print_time_step_diagnostics(df_slice, id_pair, i, start, end)\n",
    "    \n",
    "# Log‐spaced (hour→day→week) bins in minutes\n",
    "time_bins   = [0, 60, 180, 360, 720, 1440, 2880, 5760, 11520, 23040, 30240]\n",
    "time_labels = [\"0-1h\",\"1-3h\",\"3-6h\",\"6-12h\",\"12-24h\",\"1-2d\",\"2-4d\",\"4-8d\",\"8-16d\",\"16-21d\"]\n",
    "\n",
    "MC_START = 901 # revise\n",
    "MC_END = 1000 # revise\n",
    "tasks = []\n",
    "for slice_idx, (win_start, win_end) in enumerate(windows):\n",
    "    tasks.append((\n",
    "        slice_idx,\n",
    "        win_start,            # new argument\n",
    "        win_end,              # new argument\n",
    "        df_full,\n",
    "        id_pair,\n",
    "        raster_paths,\n",
    "        slice_output_base,\n",
    "        1000,                    # M_C\n",
    "        time_bins,\n",
    "        time_labels,\n",
    "        MC_START,\n",
    "        MC_END\n",
    "    ))\n",
    "\n",
    "n_workers = min(os.cpu_count()-1, len(tasks))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as exe:\n",
    "    futures = {\n",
    "        exe.submit(workers_null_model.process_slice_reverse_time, *args): args[0]\n",
    "        for args in tasks\n",
    "    }\n",
    "\n",
    "    for fut in as_completed(futures):\n",
    "        slice_idx = futures[fut]\n",
    "        try:\n",
    "            sl, tot_ev, tot_pr = fut.result()\n",
    "            print(f\"[Slice {sl:04d}] ✅ done — events: {tot_ev}, pairs: {tot_pr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Slice {slice_idx:04d}] ⚠ failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1480ee3-2c9f-4124-a904-489169a307af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
